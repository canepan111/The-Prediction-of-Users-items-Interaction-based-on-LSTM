{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.Since the 'Final_Project_Group13.ipynb' model did not use early stopping, it was challenging to control the learning rate and epoch size to optimize the model. Therefore, in this model, I employed a strategy that combines dynamic learning rate adjustments with epoch number selection. I used a scheduler to automatically adjust the learning rate during the training process, and implemented an early stopping strategy to automatically determine the optimal number of epochs, thus optimizing the model performance.\n",
    "\n",
    "##### 2.The code and strategies for this experiment were completed solely by Doucheng PAN!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load data\n",
    "data_path = 'wikipedia.csv'\n",
    "\n",
    "# Read CSV file without automatically parsing the column names, allowing pandas to freely read all columns\n",
    "data = pd.read_csv(data_path, header=None)\n",
    "\n",
    "# Create a list of column names: The first four column names are fixed, the rest are dynamically generated based on the number of features\n",
    "column_names = ['user_id', 'item_id', 'timestamp', 'state_label'] + [f'feature_{i}' for i in range(1, data.shape[1] - 3)]\n",
    "\n",
    "# Assign column names to the DataFrame\n",
    "data.columns = column_names\n",
    "# Ensure the data is sorted by timestamp\n",
    "data.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Split the dataset\n",
    "train_size = int(len(data) * 0.6) # 60% of the data for training\n",
    "val_size = int(len(data) * 0.2) # 20% of the data for validation\n",
    "\n",
    "train_data = data[:train_size]\n",
    "val_data = data[train_size:train_size+val_size]\n",
    "test_data = data[train_size+val_size:]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "train_features = scaler.fit_transform(train_data.drop(['state_label'], axis=1))  # Fit and transform training data\n",
    "val_features = scaler.transform(val_data.drop(['state_label'], axis=1))  # Transform validation data\n",
    "test_features = scaler.transform(test_data.drop(['state_label'], axis=1))  # Transform test data\n",
    "\n",
    "# Extract labels\n",
    "train_labels = train_data['state_label'].values\n",
    "val_labels = val_data['state_label'].values\n",
    "test_labels = test_data['state_label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package data into a format suitable for LSTM input\n",
    "train_dataset = TensorDataset(torch.tensor(train_features, dtype=torch.float32), torch.tensor(train_labels, dtype=torch.float32))\n",
    "val_dataset = TensorDataset(torch.tensor(val_features, dtype=torch.float32), torch.tensor(val_labels, dtype=torch.float32))\n",
    "test_dataset = TensorDataset(torch.tensor(test_features, dtype=torch.float32), torch.tensor(test_labels, dtype=torch.float32))\n",
    "\n",
    "# Use DataLoader to handle the datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)  # Training data loader with shuffling\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)      # Validation data loader without shuffling\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)    # Test data loader without shuffling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, device):\n",
    "        \"\"\"\n",
    "        Initialize the LSTMModel with necessary layers and parameters.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): The number of input features per time step.\n",
    "            hidden_dim (int): The number of features in the hidden state of the LSTM.\n",
    "            num_layers (int): The number of stacked LSTM layers.\n",
    "            output_dim (int): The number of output features.\n",
    "            device (torch.device): The device (CPU or GPU) the model will run on.\n",
    "        \"\"\"\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.device = device  # Device where the model will be placed\n",
    "        self.hidden_dim = hidden_dim  # Number of units in the LSTM hidden layer\n",
    "        self.num_layers = num_layers  # Number of LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)  # The LSTM module\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)  # Linear layer to map from hidden state to output\n",
    "        self.to(device)  # Move the model to the specified device\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor containing sequences for the LSTM.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output of the model after processing input tensor x.\n",
    "        \"\"\"\n",
    "        # Initialize hidden state and cell state for LSTM with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(self.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(self.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # pass the initial hidden and cell states\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])  # Apply the linear layer to the hidden state of the last time step\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    EarlyStopping utility to stop training when the validation loss has not improved after\n",
    "    a certain number of epochs (patience).\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=5, verbose=False):\n",
    "        \"\"\"\n",
    "        Initializes the EarlyStopping callback.\n",
    "        \n",
    "        Args:\n",
    "            patience (int): Number of epochs to wait after last time validation loss improved.\n",
    "                            Default: 5\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement.\n",
    "                            Default: False\n",
    "        \"\"\"\n",
    "        self.patience = patience  # How long to wait after last time validation loss improved.\n",
    "        self.verbose = verbose  # If True, prints messages about validation loss improvement.\n",
    "        self.counter = 0  # Counter to keep track of how long since last improvement\n",
    "        self.best_score = None  # Best score seen in the validation data\n",
    "        self.early_stop = False  # Flag to signal early stopping\n",
    "        self.val_loss_min = float('inf')  # Minimum validation loss seen so far\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        \"\"\"\n",
    "        Call method that executes the early stopping logic.\n",
    "        \n",
    "        Args:\n",
    "            val_loss (float): Current epoch's validation loss.\n",
    "            model (torch.nn.Module): The model being trained.\n",
    "        \"\"\"\n",
    "        if self.best_score is None:\n",
    "            # Set the initial 'best' to the first validation loss we see\n",
    "            self.best_score = val_loss\n",
    "        elif val_loss > self.best_score:\n",
    "            # If the validation loss is worse, increment the counter\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                # If counter exceeds patience, set early stopping flag\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            # If validation loss improves, reset counter and update best score\n",
    "            self.best_score = val_loss\n",
    "            self.counter = 0\n",
    "            if self.verbose:\n",
    "                # Optionally print a message about the improvement\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model...')\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'checkpoint_model.pth')\n",
    "            self.val_loss_min = val_loss  # Update the minimum validation loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the device to use based on CUDA availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model parameters\n",
    "input_dim = train_features.shape[1]  # The number of input features per time step\n",
    "hidden_dim = 128  # Number of hidden units in each LSTM layer, can be adjusted based on model complexity\n",
    "num_layers = 3  # Number of LSTM layers in the model\n",
    "output_dim = 1  # Output dimension for binary classification\n",
    "\n",
    "# Instantiate the model with the specified parameters and device\n",
    "model = LSTMModel(input_dim, hidden_dim, num_layers, output_dim, device)\n",
    "\n",
    "# Define the loss function and optimizer for training\n",
    "criterion = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy loss with logits for binary classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with a learning rate of 0.001\n",
    "\n",
    "# Set up a learning rate scheduler that reduces the learning rate when a plateau in validation loss is detected\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "# 'min' mode means the scheduler will reduce the LR when the monitored quantity stops decreasing\n",
    "# patience=5 means we wait 5 epochs with no improvement before reducing the learning rate\n",
    "# factor=0.5 reduces the learning rate by multiplying it with 0.5\n",
    "# verbose=True will print a message whenever the scheduler updates the learning rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Val Loss: 0.0093, Val AUC: 0.8385\n",
      "Epoch 2, Val Loss: 0.0078, Val AUC: 0.8384\n",
      "Validation loss decreased (inf --> 0.007814).  Saving model...\n",
      "Epoch 3, Val Loss: 0.0077, Val AUC: 0.8443\n",
      "Validation loss decreased (0.007814 --> 0.007735).  Saving model...\n",
      "Epoch 4, Val Loss: 0.0081, Val AUC: 0.8675\n",
      "Epoch 5, Val Loss: 0.0078, Val AUC: 0.8390\n",
      "Epoch 6, Val Loss: 0.0080, Val AUC: 0.8429\n",
      "Epoch 7, Val Loss: 0.0084, Val AUC: 0.8431\n",
      "Epoch 8, Val Loss: 0.0082, Val AUC: 0.8279\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "# Early stopping setup to prevent overfitting and stop training when the validation loss does not improve\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "\n",
    "num_epochs = 50  # Setting a relatively high number of epochs, early stopping will determine the actual number\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the device\n",
    "        inputs = inputs.unsqueeze(1)  # Add a sequence length dimension (LSTM expects three-dimensional input)\n",
    "        optimizer.zero_grad()  # Clear gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs.squeeze(), labels)  # Compute loss\n",
    "        loss.backward()  # Backpropagate the error\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    valid_loss = 0\n",
    "    valid_preds, valid_labels = [], []\n",
    "    with torch.no_grad():  # Disable gradient computation during validation\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = inputs.unsqueeze(1)  # Ensure input has sequence length dimension\n",
    "            outputs = model(inputs)\n",
    "            valid_preds.extend(outputs.squeeze().tolist())\n",
    "            valid_labels.extend(labels.tolist())\n",
    "            loss = criterion(outputs.squeeze(), labels)  # Compute loss\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "    valid_loss /= len(val_loader)  # Average the validation loss\n",
    "    valid_auc_score = roc_auc_score(valid_labels, valid_preds)  # Calculate AUC for validation set\n",
    "    print(f'Epoch {epoch+1}, Val Loss: {valid_loss:.4f}, Val AUC: {valid_auc_score:.4f}')\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler.step(valid_loss)  # Adjust learning rate based on validation loss\n",
    "\n",
    "    # Early stopping check\n",
    "    early_stopping(valid_loss, model)  # Check if early stopping criteria are met\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")  # Stop training if the early stopping condition is satisfied\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC: 0.8598\n"
     ]
    }
   ],
   "source": [
    "# Load the best model saved during training\n",
    "model.load_state_dict(torch.load('checkpoint_model.pth'))\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_preds, test_labels = [], []  # Lists to store predictions and labels\n",
    "with torch.no_grad():  # Disable gradient computation\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the device\n",
    "        inputs = inputs.unsqueeze(1)  # Ensure input has sequence length dimension for LSTM\n",
    "        outputs = model(inputs)  # Forward pass to get output from the model\n",
    "        test_preds.extend(outputs.squeeze().tolist())  # Collect predictions\n",
    "        test_labels.extend(labels.tolist())  # Collect actual labels\n",
    "\n",
    "test_auc_score = roc_auc_score(test_labels, test_preds)  # Calculate AUC score for the test data\n",
    "print(f'Test AUC: {test_auc_score:.4f}')  # Print the AUC score to evaluate model performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
